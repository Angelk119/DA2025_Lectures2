{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge 12 ‚Äî Intro to Logistic Regression\n",
    "\n",
    "**Hook (Attention Grabber)**  \n",
    "> ‚ÄúIf an app told a restaurant it has an 80% chance of getting an **A** on inspection, would you trust it?‚Äù\n",
    "\n",
    "**Learning Goals**\n",
    "- Show why **linear regression** is a bad fit for a **binary (0/1)** target.\n",
    "- Fit a **one-feature logistic regression** and interpret probabilities.\n",
    "- Extend to a **two-feature logistic model with standardized inputs**.\n",
    "- Communicate results using **AWES** and discuss **ethics & people impact**.\n",
    "\n",
    "**Data:** June 1, 2025 - Nov 4, 2025 Restaurant Health Inspection\n",
    "\n",
    "[Restaurant Health Inspection](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor Guidance\n",
    "\n",
    "**Hint: Use the Lecture Deck, Canvas Reading, and Docs to help you with the code**\n",
    "\n",
    "Use this guide live; students implement below.\n",
    "\n",
    "**Docs (quick links):**\n",
    "- Train/Test Split ‚Äî scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- LinearRegression ‚Äî scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- LogisticRegression ‚Äî scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- StandardScaler ‚Äî scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- accuracy_score ‚Äî scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "- corr ‚Äî pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "### Pseudocode Plan (Linear vs Logistic + Scaling)\n",
    "1) **Load CSV** ‚Üí preview shape/columns; keep needed fields.  \n",
    "2) **Engineer binary Y**: `is_A = 1 if grade == 'A' else 0`.  \n",
    "3) **Pick numeric X**:  \n",
    "   - **X1:** `score` (inspection score; lower is better)  \n",
    "   - **X2:** `critical_num = 1 if critical_flag == 'Critical' else 0` (for extension)  \n",
    "4) **Split** ‚Üí `X_train, X_test, y_train, y_test` (70/30, stratify by Y, fixed random_state).  \n",
    "5) **Model A (Incorrect)** ‚Üí **LinearRegression** on Y~X1:  \n",
    "   - Report **MSE**, **R¬≤**, count predictions **<0 or >1**,  \n",
    "6) **Model B (Correct)** ‚Üí **LogisticRegression** on Y~X1:  \n",
    "   - Report **Accuracy**\n",
    "7) **Visual (OPTIONAL)** ‚Üí scatter Y vs X1 with **linear line** vs **logistic sigmoid** curve  \n",
    "8) **Extension** ‚Üí scale X1+X2 with **StandardScaler**; fit **LogisticRegression**:  \n",
    "   - Compare **Accuracy** to one-feature logistic  \n",
    "9) **Interpret** ‚Üí 2‚Äì3 sentences on why linear fails and how logistic fixes it  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Do ‚Äî Student Section\n",
    "Work in pairs. Comment your choices briefly. Keep code simple‚Äîonly coerce the columns you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Imports and Plot Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.4f}')\n",
    "#Some of these imports may not be used but they are just here just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 ‚Äî Load CSV & Preview\n",
    "- Point to your New York City Restaurant Inspection Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((291278, 27),\n",
       " ['CAMIS',\n",
       "  'DBA',\n",
       "  'BORO',\n",
       "  'BUILDING',\n",
       "  'STREET',\n",
       "  'ZIPCODE',\n",
       "  'PHONE',\n",
       "  'CUISINE DESCRIPTION',\n",
       "  'INSPECTION DATE',\n",
       "  'ACTION',\n",
       "  'VIOLATION CODE',\n",
       "  'VIOLATION DESCRIPTION',\n",
       "  'CRITICAL FLAG',\n",
       "  'SCORE',\n",
       "  'GRADE',\n",
       "  'GRADE DATE',\n",
       "  'RECORD DATE',\n",
       "  'INSPECTION TYPE',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Community Board',\n",
       "  'Council District',\n",
       "  'Census Tract',\n",
       "  'BIN',\n",
       "  'BBL',\n",
       "  'NTA',\n",
       "  'Location'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gabriel/Desktop/marcy/DA2025_Lectures2/Mod6/data/DOHMH_New_York_City_Restaurant_Inspection_Results_20251104 copy.csv', low_memory=False)\n",
    "df = df.dropna()\n",
    "(df.shape, df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Clean and Engineer Features\n",
    "- Make sure `SCORE` is numeric and do any other data type clean-up \n",
    "- Engineer binary target variable (Y) based on instructor guidance above `is_A`\n",
    "- Engineer binary predictor (X) based on instructor guidance above `critical_num`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.replace('NAN', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.dropna(subset=['GRADE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAMIS', 'DBA', 'BORO', 'BUILDING', 'STREET', 'ZIPCODE', 'PHONE', 'CUISINE DESCRIPTION', 'INSPECTION DATE', 'ACTION', 'VIOLATION CODE', 'VIOLATION DESCRIPTION', 'CRITICAL FLAG', 'SCORE', 'GRADE', 'GRADE DATE', 'RECORD DATE', 'INSPECTION TYPE', 'Latitude', 'Longitude', 'Community Board', 'Council District', 'Census Tract', 'BIN', 'BBL', 'NTA', 'Location', 'is_A', 'critical_num']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRITICAL' 'NOT APPLICABLE' 'NOT CRITICAL']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df['CRITICAL FLAG'].unique()[:10])  # See sample unique values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18       13.0000\n",
       "19        0.0000\n",
       "36       13.0000\n",
       "37        0.0000\n",
       "54        0.0000\n",
       "           ...  \n",
       "291273    0.0000\n",
       "291274   40.0000\n",
       "291275   27.0000\n",
       "291276   31.0000\n",
       "291277    6.0000\n",
       "Name: SCORE, Length: 274939, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SCORE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SCORE GRADE  is_A  critical_num\n",
      "18 13.0000     A     1             1\n",
      "19  0.0000   NAN     0             0\n",
      "36 13.0000     A     1             0\n",
      "37  0.0000     P     0             0\n",
      "54  0.0000     A     1             0\n",
      "CAMIS                      int64\n",
      "DBA                       object\n",
      "BORO                      object\n",
      "BUILDING                  object\n",
      "STREET                    object\n",
      "ZIPCODE                  float64\n",
      "PHONE                     object\n",
      "CUISINE DESCRIPTION       object\n",
      "INSPECTION DATE           object\n",
      "ACTION                    object\n",
      "VIOLATION CODE            object\n",
      "VIOLATION DESCRIPTION     object\n",
      "CRITICAL FLAG             object\n",
      "SCORE                    float64\n",
      "GRADE                     object\n",
      "GRADE DATE                object\n",
      "RECORD DATE               object\n",
      "INSPECTION TYPE           object\n",
      "Latitude                 float64\n",
      "Longitude                float64\n",
      "Community Board          float64\n",
      "Council District         float64\n",
      "Census Tract             float64\n",
      "BIN                      float64\n",
      "BBL                      float64\n",
      "NTA                       object\n",
      "Location                  object\n",
      "is_A                       int64\n",
      "critical_num               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['SCORE'] = pd.to_numeric(df['SCORE'], errors='coerce')\n",
    "df = df.dropna(subset=['SCORE'])\n",
    "\n",
    "# Standardize GRADE and CRITICAL FLAG columns to uppercase text\n",
    "df['GRADE'] = df['GRADE'].astype(str).str.strip().str.upper()\n",
    "\n",
    "df['CRITICAL FLAG'] = df['CRITICAL FLAG'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Create binary variables \n",
    "df['is_A'] = (df['GRADE'] == 'A').astype(int)\n",
    "df['critical_num'] = (df['CRITICAL FLAG'] == 'CRITICAL').astype(int)\n",
    "\n",
    "print(df[['SCORE', 'GRADE', 'is_A', 'critical_num']].head())\n",
    "print(df.dtypes)\n",
    "newdf = df[['SCORE', 'GRADE', 'is_A', 'critical_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_A\n",
       "0    178666\n",
       "1     96273\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf['is_A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_A\n",
       "1    96273\n",
       "0    45921\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['is_A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "critical_num\n",
       "1    71601\n",
       "0    70593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['critical_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRADE\n",
       "A    96273\n",
       "B    18070\n",
       "C    12951\n",
       "N     7702\n",
       "Z     6280\n",
       "P      918\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['GRADE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "critical_num\n",
       "1    153985\n",
       "0    120954\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['critical_num'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Split Data (70/30 Stratify by Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['SCORE', 'critical_num']]\n",
    "y = df['is_A']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Model A: Linear Regression on a Binary Target (Incorrect)\n",
    "\n",
    "- Fit `is_A (Y var) ~ SCORE (X pred)` using **LinearRegression**  \n",
    "- Report **MSE**, **R¬≤**, and how many predictions fall outside [0, 1]  \n",
    "- Estimate accuracy by thresholding predictions at 0.5 (done for you but understand the code) \n",
    "\n",
    "üí° Hint:  \n",
    "`accuracy_score(y_test, (y_pred >= 0.5).astype(int))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14979399395167886\n",
      "R¬≤: 0.3417047623365893\n",
      "Predictions outside [0,1]: 8596\n",
      "Accuracy (threshold 0.5): 0.9528381950001212\n"
     ]
    }
   ],
   "source": [
    "model_a = LinearRegression()\n",
    "model_a.fit(X_train[['SCORE']], y_train)\n",
    "\n",
    "y_pred = model_a.predict(X_test[['SCORE']])\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "outside = ((y_pred < 0) | (y_pred > 1)).sum()\n",
    "acc = accuracy_score(y_test, (y_pred >= 0.5).astype(int))\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R¬≤:\", r2)\n",
    "print(\"Predictions outside [0,1]:\", outside)\n",
    "print(\"Accuracy (threshold 0.5):\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Model B: Logistic Regression (One Feature)\n",
    "\n",
    "- Fit `is_A ~ score` using **LogisticRegression**  \n",
    "- Compute predictions with `.predict()`  \n",
    "- Evaluate accuracy with `accuracy_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression - SCORE only): 0.9528381950001212\n"
     ]
    }
   ],
   "source": [
    "model_b = LogisticRegression()\n",
    "model_b.fit(X_train[['SCORE']], y_train)\n",
    "\n",
    "y_pred_b = model_b.predict(X_test[['SCORE']])\n",
    "\n",
    "acc_b = accuracy_score(y_test, y_pred_b)\n",
    "\n",
    "print(\"Accuracy (Logistic Regression - SCORE only):\", acc_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 (OPTIONAL) ‚Äî Visual Comparison: Linear vs Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 ‚Äî Logistic Regression with Two **Scaled** Features\n",
    "\n",
    "- Use `SCORE` and `critical_num` as your two X predictors that need to be scaled\n",
    "- Look at documentation above to see how you would fit a StandardScalar() object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression - Scaled SCORE + critical_num): 0.9503649281055261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[['SCORE', 'critical_num']]\n",
    "y = df['is_A']\n",
    "\n",
    "# Split data (reuse if already done)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model_c = LogisticRegression()\n",
    "model_c.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_c = model_c.predict(X_test_scaled)\n",
    "acc_c = accuracy_score(y_test, y_pred_c)\n",
    "\n",
    "print(\"Accuracy (Logistic Regression - Scaled SCORE + critical_num):\", acc_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Share ‚Äî Reflection & Wrap-Up\n",
    "\n",
    "Write **two short paragraphs** (4‚Äì6 sentences each). Be specific and use evidence from your notebook.\n",
    "\n",
    "1Ô∏è‚É£ **How do you know Linear Regression was a poor model choice for this task?**  \n",
    "Describe what you observed in your results or plots that showed it didn‚Äôt work well for a binary outcome.  \n",
    "Consider: Were predictions outside 0‚Äì1? Did the fit look wrong? What happened when you used 0.5 as a cutoff?  \n",
    "Connect this to the idea that classification models should output probabilities between 0 and 1.\n",
    "\n",
    "2Ô∏è‚É£ **When should we scale features in logistic regression (and when not to)?**  \n",
    "Explain what scaling does, and why it might (or might not) matter for different kinds of features.  \n",
    "Use this project to reason through whether `score` and `critical_num` needed scaling.  \n",
    "Hint: Think about what ‚Äúcontinuous‚Äù vs ‚Äúbinary‚Äù means for scaling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression was a poor choice for this binary target because it produced predictions outside the 0‚Äì1 range and had a low R¬≤ value. The scatterplot and metrics showed that the model couldn‚Äôt properly capture the ‚Äúyes/no‚Äù pattern of grades. When predictions were thresholded at 0.5, accuracy was weak and inconsistent. This happens because linear regression doesn‚Äôt constrain outputs to valid probabilities, making it unsuitable for classification tasks.\n",
    "\n",
    "We scale features in logistic regression when predictors have very different ranges or units. Scaling helps the model converge faster and ensures regularization treats features fairly. In this project, SCORE is continuous and benefits from scaling, while critical_num is binary and doesn‚Äôt strictly need it. Still, scaling both is fine and keeps the model balanced when training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
